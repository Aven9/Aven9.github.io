<!DOCTYPE html><html lang="en-us"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><title>GAN in Editing Portrait | Aven's Blog</title><meta name="description"><meta name="generator" content="Aven's Blog"><meta name="author" content="Wenjie Zhang"><meta name="keywords" content="Developer, Computer Science"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=1,user-scalable=0"><link rel="stylesheet" type="text/css" href="/styles/screen.css"><link rel="apple-touch-icon" sizes="57x57" href="/images/apple-touch-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/images/apple-touch-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/images/apple-touch-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/images/apple-touch-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/images/apple-touch-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/images/apple-touch-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/images/apple-touch-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/images/apple-touch-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-180x180.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/images/favicon-96x96.png"><link rel="icon" type="image/png" sizes="128x128" href="/images/favicon-128.png"><link rel="icon" type="image/png" sizes="196x196" href="/images/favicon-196x196.png"><meta name="msapplication-TileColor" content="#FFFFFF"><meta name="msapplication-TileImage" content="mstile-144x144.png"><meta name="msapplication-square70x70logo" content="mstile-70x70.png"><meta name="msapplication-square150x150logo" content="mstile-150x150.png"><meta name="msapplication-wide310x150logo" content="mstile-310x150.png"><meta name="msapplication-square310x310logo" content="mstile-310x310.png"></head><body itemscope itemtype="https://schema.org/WebPage"><header itemscope itemtype="https://schema.org/WPHeader"><div class="logo"></div><h1><a href="/" alt="Aven's Blog" title="Aven's Blog" itemprop="headline">Aven's Blog</a></h1><p itemprop="description"></p><nav itemscope itemtype="https://schema.org/SiteNavigationElement"><ul><li itemprop="name"><a href="/ " alt="home" title="home" itemprop="url">home</a></li><li itemprop="name"><a href="/articles" alt="articles" title="articles" itemprop="url">articles</a></li><li itemprop="name"><a href="/author" alt="author" title="author" itemprop="url">author</a></li></ul></nav><div class="space"></div></header><main itemscope itemtype="https://schema.org/Blog"><article class="full"><h1 itemprop="headline" class="post-heading">GAN in Editing Portrait</h1><span class="page-tag-anchor"><a href="/tags/Computer Vision" itemprop="url">#Computer Vision
&nbsp;&nbsp;</a></span><span class="post-meta"></span><br><br><p>前段时间看了一篇论文，是生成对抗网络在人像编辑方面的应用，以这个模型为核心跟队友摸鱼写了一个以用户的直接涂改，包括表示修改区域的蒙板、修改笔迹、颜色作为输入的智能人像编辑iPad应用。以下是对文章原理和提供方法的一个简述。</p>
<p>[TOC]</p>
<h1 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h1><h2 id="GAN原理简述"><a href="#GAN原理简述" class="headerlink" title="GAN原理简述"></a>GAN原理简述</h2><p>顾名思义，GAN，即生成对抗网络，是由生成器和判别器构成的一种深度学习模型。以人像编辑软件为例，图像生成器的目的是接收用户的涂改作为输入，合成修改后的图片。图像判定器负责判断合成的图片看上去是真的还是假的。而生成模型的训练，就是通过生成器生成图片，判别器做出判断，生成器改进，继续生成图片……直到判别器分辨不出接收到的是一张真实的图片还是由用户提交的修改的图片。</p>
<h2 id="深度学习在人像编辑的应用"><a href="#深度学习在人像编辑的应用" class="headerlink" title="深度学习在人像编辑的应用"></a>深度学习在人像编辑的应用</h2><h3 id="工作简述"><a href="#工作简述" class="headerlink" title="工作简述"></a>工作简述</h3><p>深度学习已经可以应用于恢复图像中被擦除的部分。最经典的方法就是使用一个方形的蒙板，使用编码-解码生成器恢复蒙板区域的图像，图像是否真实的判断由一个全局本地判别器来判定。但是这种方法的缺点就是较低的图像解析程度，并且由于方形的蒙板导致生成的图像在蒙板区域有一个尴尬的边界。除此之外，生成器不能接收用户的输入，降低了用户友好度。</p>
<p>其它改进的方式包括Deepfillv2(引入用户输入来恢复缺失部分)，Guided-Inpating(使用其它图片的部分来恢复被删除的部分)。一些其它的工作如Ideepcolor提出了一个能够接受用户输入作为参考，创建彩色图像；FaceShop可接受笔画和颜色同时作为用户输入。但这些方法都有着不同程度的用户交互短板，甚至不尽人意的完成效果。</p>
<p>为克服这些缺点，这篇文章提出了一种完全卷积神经网络的SC-FEGAN，用于进行端到端训练。该网络利用SN-patchGAN判别器克服尴尬的边界；利用通用的GAN损失和新增的style损失来编辑脸部图像；复合出的图片具有高质量的真实性；具有高度自由的用户输入形式。</p>
<h3 id="图像翻译"><a href="#图像翻译" class="headerlink" title="图像翻译"></a>图像翻译</h3><p>用于图像翻译的GAN是在用于学习在两个数据集之间进行图像区域转换。该数据集由一对图像组成，可用于创建将分割标签转换为原始图像，或将草图转换为图像，或将黑白图像转换为彩色图像。但是该系统要求图像和目标图像必须成对存在于训练数据集中，以便学习域之间的转换。给定目标域而没有目标图像，则在原始域中转换图像时，目标域中会存在虚拟结果。如果虚拟结果再次反转，则反转的结果必须是原始图像。因此，转换任务需要两个生成器。</p>
<h3 id="图像完成"><a href="#图像完成" class="headerlink" title="图像完成"></a>图像完成</h3><p>图像完成领域面临两个主要挑战：</p>
<ol>
<li>填充图像的已删除区域</li>
<li>在恢复区域中正确反映用户输入。</li>
</ol>
<h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><h2 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h2><p>适当的训练数据对于提高网络的训练性能和提高对用户输入的响应能力非常重要。为了训练模型，使用了CelebA-HQ 数据集。预处理步骤如下：</p>
<ol>
<li>随机选取包含29000张图片的集合作为训练数据集，1000张图片作为测试数据集。</li>
<li>在获得草图和颜色数据集之前，我们将图像调整为512×512像素。</li>
<li>为了更好地表达人脸图像中眼睛的复杂性，我们使用了基于眼睛位置的自由蒙板来训练网络</li>
<li>通过使用自由形式的蒙版和面部分割GFC创建了适当的草图域和颜色域。</li>
</ol>
<p>关键设计如下：</p>
<ol>
<li><p>带有眼睛位置的自由蒙版</p>
<p>我们使用了类似于Deepfillv2的方法，即采用蒙板的方式，然后利用用户在蒙板上的输入生成不完全的图像，此外，在用面部图像训练时，随机使用以眼睛位置为起点的自由绘制蒙版来表达眼睛的复杂部分。另外，头发的蒙板采用了GFC，细节如下算法表述的那样：</p>
<pre><code>maxDraw, maxLine, maxAngle, maxLength are hyper
parameters
GFCHair is the GFC for get hair mask of input image Mask=zeros(inputSize,inputSize) HairMask=GFCHair(IntputImage) numLine=random.range(maxDraw)
for i=0 to numLine do
startX = random.range(inputSize) startY = random.range(inputSize) startAngle = random.range(360) numV = random.range(maxLine) for j=0 to numV do
angleP = random.range(-maxAngle,maxAngle) if j is even then
angle = startAngle+angleP
else
angle = startAngle+angleP+180
end if
length = random.range(maxLength)
Draw a line on Mask from point (startX, startY) with angle and length.
startX = startX + length * sin(angle)
startY = stateY + length * cos(angle)
end for
Draw a line on Mask from eye postion randomly.
end for
Mask = Mask + HairMask (randomly)</code></pre></li>
<li><p>草图、颜色域</p>
<p>1）用HED边缘检测器生成草图数据，该数据对应于用户输入以修改面部图像。</p>
<p>2）之后，使曲线平滑并消除了小边缘。为了创建色域数据，首先通过应用大小为3的中值过滤，然后应用20个双边过滤器来创建模糊图像。</p>
<p>3）之后，使用GFC分割人脸，并将每个分割的部分替换为相应部分的中间颜色。</p>
<p>4）在为色域创建数据时，未应用直方图均衡，以避免光反射和阴影污染颜色。但是，由于用户更愿意在草图域中表达面部的所有部分，而不受光干扰引起的模糊影响，因此在从主体数据创建草图时使用了直方图均衡化。</p>
</li>
</ol>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>用于图像补充的生成器网络基于编码器-解码器架构，判别器网络则是基于SN-patchGAN结构。这样的网络结构能够快速训练以达稳定，并且生成高质量的大小为512<em>512的合成图片。训练时，生成器和判别器同时进行。*</em>生成器接收带用户输入的不完整图片，输出带RGB通道的图片，并将蒙板区域的输出图像插入到输入图片中，由此来创建一张完整的图片。**详细网络结构如下图所示：</p>
<p><img src="./Image-completion-with-generative-adversarial-networks/structure.png" alt="structure"></p>
<h3 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h3><p>生成器基于U-net，即编码器-解码器结构。编码器接收尺寸为512×512×9的输入张量，即一个不完整的RGB通道图像，其中有一个要编辑的去除区域，一个二进制草图（描述了去除部分的结构），一个RGB色彩笔划图，一个二进制蒙版和噪音，见下图</p>
<p><img src="./Image-completion-with-generative-adversarial-networks/input.png" alt="input"></p>
<p>编码器使用2个步幅内核卷积对输入进行7次下采样，然后在上采样之前进行膨胀卷积。解码器使用转置卷积进行上采样。然后，添加了跳过连接以允许以相同的空间分辨率与上一层连接。除了输出层使用tanh函数外，在每层之后都使用了泄漏的ReLU激活函数。总体而言，生成器由16个卷积层组成，网络的输出是输入大小相同（512×512）的RGB图像。在将损失函数应用到输入之前，我们用输入图像替换了蒙板之外的图像的其余部分。这种替换使生成器仅在已编辑区域上受到训练。生成器接受了每像素损失，感知损失，样式损失和总方差损失这些损失训练，还使用通用GAN损失函数。</p>
<h3 id="判别器"><a href="#判别器" class="headerlink" title="判别器"></a>判别器</h3><p>判别器具有SN-PatchGAN 结构，它的特性是没有对GAN损失应用ReLu函数。另外还使用了3×3大小的卷积内核并应用了梯度损失。整体损失函数计算如下所示：<br>$$<br>L_{GSN} = -IE[D(I_{comp})]<br>$$</p>
<p>$$<br>L_G = L_{per-pixel}+\sigma L_{percent}+\beta L_{G_SN}+ \gamma (L_{style}(I_{gen})+L_{style}(I_{comp}))+\upsilon L_{t\upsilon}+\epsilon IE[D(I_{gt})^2]<br>$$</p>
<p>$$<br>L_D = IE[1-D(I_{gt})]+IE[1+D(I_{comp})]+\theta L_{GP}<br>$$</p>
<p>生成器用$$L_G$$训练，判别器用$$L_D$$训练，$$D(I)$$是判别器的输出，其中$I$为输入；在编辑较大的区域比如头发时，损失$$L_{style}$$和$$L_{percent}$$十分关键，</p>
<h2 id=""><a href="#" class="headerlink" title=""></a></h2></article><br><br><span class="next-post"><a href="/2020/01/21/hello-world/" itemprop="url">Older Post ⇒</a></span><br><br><br></main></body></html>